{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def load_data(image_files, json_files):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for img_file, json_file in zip(image_files, json_files):\n",
    "        # Load image\n",
    "        img = Image.open(img_file)\n",
    "        img = img.convert('RGB')\n",
    "        images.append(np.array(img))\n",
    "        \n",
    "        # Load JSON\n",
    "        with open(json_file, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        # Assuming JSON data is a list of coordinates [{'x': .., 'y': ..}, ...]\n",
    "        labels.append(data)\n",
    "    \n",
    "    return np.array(images), labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (100,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 44\u001b[0m\n\u001b[0;32m     42\u001b[0m img_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../images\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Update this to the correct path\u001b[39;00m\n\u001b[0;32m     43\u001b[0m json_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../json_labeling\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Update this to the correct path\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m load_data(img_folder, json_folder)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Split data into training and testing sets\u001b[39;00m\n\u001b[0;32m     47\u001b[0m train_images, test_images, train_labels, test_labels \u001b[38;5;241m=\u001b[39m train_test_split(images, labels, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "Cell \u001b[1;32mIn[68], line 38\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(img_folder, json_folder)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Normalize images\u001b[39;00m\n\u001b[0;32m     37\u001b[0m images \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(images) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[1;32m---> 38\u001b[0m labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(labels)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m images, labels\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (100,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_data(img_folder, json_folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for i in range(1, 101):  # Assuming you have 100 images and JSON files\n",
    "        img_path = img_folder + f'/image_{i}.png' #os.path.join(img_folder, f'image_{i}.png')\n",
    "        json_path = json_folder + f'/label_{i}.json' #os.path.join(json_folder, f'label_{i}.json')\n",
    "\n",
    "        # Load image\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            print(f\"Error loading image: {img_path}\")\n",
    "            continue  # Skip this image if it cannot be loaded\n",
    "        image = cv2.resize(image, (224, 224))  # Resize image to match the CNN input\n",
    "        images.append(image)\n",
    "\n",
    "        # Load JSON data\n",
    "        with open(json_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            boxes = []\n",
    "            for item in data['boxes']:\n",
    "                if item['label'] == 'ball':\n",
    "                    x = float(item['x']) / data['width']\n",
    "                    y = float(item['y']) / data['height']\n",
    "                    width = float(item['width']) / data['width']\n",
    "                    height = float(item['height']) / data['height']\n",
    "                    boxes.append([x, y, width, height])\n",
    "            labels.append(boxes)\n",
    "\n",
    "    # Normalize images\n",
    "    images = np.array(images) / 255.0\n",
    "    labels = np.array(labels)\n",
    "    return images, labels\n",
    "\n",
    "# Load dataset\n",
    "img_folder = '../images'  # Update this to the correct path\n",
    "json_folder = '../json_labeling'  # Update this to the correct path\n",
    "images, labels = load_data(img_folder, json_folder)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(images, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def load_images(image_dir):\n",
    "    image_files = [os.path.join(image_dir, f) for f in sorted(os.listdir(image_dir)) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    images = []\n",
    "    for img_file in image_files:\n",
    "        img = Image.open(img_file).convert('RGB')  # Convert to RGB to ensure 3 color channels\n",
    "        images.append(np.array(img))\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def load_labels(json_dir):\n",
    "    json_files = [os.path.join(json_dir, f) for f in sorted(os.listdir(json_dir)) if f.endswith('.json')]\n",
    "    labels = []\n",
    "    for json_file in json_files:\n",
    "        with open(json_file, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        # Access the 'boxes' key, which contains the list of objects\n",
    "        bounding_boxes = []\n",
    "        for obj in data['boxes']:\n",
    "            # Ensure values are treated as floats since they appear as strings in the JSON\n",
    "            x = float(obj['x'])\n",
    "            y = float(obj['y'])\n",
    "            width = float(obj['width'])\n",
    "            height = float(obj['height'])\n",
    "            bounding_boxes.append([x, y, x + width, y + height])\n",
    "        labels.append(bounding_boxes)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve2d(image, kernel, stride=1, padding=1):\n",
    "    # Adding padding to the input image\n",
    "    image_padded = np.pad(image, [(padding, padding), (padding, padding), (0, 0)], mode='constant', constant_values=0)\n",
    "    kernel_height, kernel_width = kernel.shape[0], kernel.shape[1]\n",
    "    output_height = (image.shape[0] - kernel_height + 2 * padding) // stride + 1\n",
    "    output_width = (image.shape[1] - kernel_width + 2 * padding) // stride + 1\n",
    "    \n",
    "    # Initialize the output feature maps\n",
    "    output = np.zeros((output_height, output_width, image.shape[2]))\n",
    "    \n",
    "    # Perform convolution\n",
    "    for z in range(image.shape[2]):  # loop over the channels\n",
    "        for y in range(output_height):\n",
    "            for x in range(output_width):\n",
    "                output[y, x, z] = np.sum(\n",
    "                    image_padded[y*stride:y*stride+kernel_height, x*stride:x*stride+kernel_width, z] * kernel[:, :, z])\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(feature_map):\n",
    "    # Applying ReLU activation element-wise\n",
    "    relu_out = np.maximum(0, feature_map)\n",
    "    return relu_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pooling(feature_map, size=2, stride=2):\n",
    "    # Initialize the output shape\n",
    "    pool_out_height = (feature_map.shape[0] - size) // stride + 1\n",
    "    pool_out_width = (feature_map.shape[1] - size) // stride + 1\n",
    "    output = np.zeros((pool_out_height, pool_out_width, feature_map.shape[2]))\n",
    "    \n",
    "    for z in range(feature_map.shape[2]):\n",
    "        for r in range(pool_out_height):\n",
    "            for c in range(pool_out_width):\n",
    "                output[r, c, z] = np.max(feature_map[r*stride:r*stride+size, c*stride:c*stride+size, z])\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(predictions, targets):\n",
    "    return ((predictions - targets) ** 2).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(d_out, input, weight, bias):\n",
    "    d_input = np.zeros_like(input)\n",
    "    d_weight = np.zeros_like(weight)\n",
    "    d_bias = np.zeros_like(bias)\n",
    "    \n",
    "    # Padding the input and input gradient for border cases\n",
    "    padded_input = np.pad(input, pad_width=((1, 1), (1, 1), (0, 0)), mode='constant', constant_values=0)\n",
    "    padded_d_input = np.pad(d_input, pad_width=((1, 1), (1, 1), (0, 0)), mode='constant', constant_values=0)\n",
    "    \n",
    "    for i in range(input.shape[0]):\n",
    "        for j in range(input.shape[1]):\n",
    "            region = padded_input[i:i+3, j:j+3]\n",
    "            d_out_ij = d_out[i, j]\n",
    "            d_weight += region * d_out_ij\n",
    "            d_bias += d_out_ij\n",
    "            padded_d_input[i:i+3, j:j+3] += weight * d_out_ij\n",
    "    \n",
    "    d_input = padded_d_input[1:-1, 1:-1]  # Remove padding\n",
    "    return d_input, d_weight, d_bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(input_image, weights, biases):\n",
    "    # Example architecture: [Conv -> ReLU -> Pool] * 2 -> Flatten -> Dense (Fully Connected)\n",
    "    # Conv Layer 1\n",
    "    conv1 = convolve2d(input_image, weights['conv1'], stride=1, padding=1) + biases['conv1']\n",
    "    relu1 = relu(conv1)\n",
    "    pool1 = max_pooling(relu1, size=2, stride=2)\n",
    "    \n",
    "    # Conv Layer 2\n",
    "    conv2 = convolve2d(pool1, weights['conv2'], stride=1, padding=1) + biases['conv2']\n",
    "    relu2 = relu(conv2)\n",
    "    pool2 = max_pooling(relu2, size=2, stride=2)\n",
    "    \n",
    "    # Flattening the output from the last pooling layer\n",
    "    flat = pool2.reshape(-1)\n",
    "    \n",
    "    # Fully Connected Layer\n",
    "    fc = np.dot(flat, weights['fc1']) + biases['fc1']\n",
    "    output = relu(fc)  # Output layer activation could be softmax or another, depending on task\n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(d_out, input, weight, bias):\n",
    "    d_input = np.zeros_like(input)\n",
    "    d_weight = np.zeros_like(weight)\n",
    "    d_bias = np.zeros_like(bias)\n",
    "    \n",
    "    # Padding the input and input gradient for border cases\n",
    "    padded_input = np.pad(input, pad_width=((1, 1), (1, 1), (0, 0)), mode='constant', constant_values=0)\n",
    "    padded_d_input = np.pad(d_input, pad_width=((1, 1), (1, 1), (0, 0)), mode='constant', constant_values=0)\n",
    "    \n",
    "    for i in range(input.shape[0]):\n",
    "        for j in range(input.shape[1]):\n",
    "            region = padded_input[i:i+3, j:j+3]\n",
    "            d_out_ij = d_out[i, j]\n",
    "            d_weight += region * d_out_ij\n",
    "            d_bias += d_out_ij\n",
    "            padded_d_input[i:i+3, j:j+3] += weight * d_out_ij\n",
    "    \n",
    "    d_input = padded_d_input[1:-1, 1:-1]  # Remove padding\n",
    "    return d_input, d_weight, d_bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(weights, biases, d_weights, d_biases, learning_rate):\n",
    "    weights -= learning_rate * d_weights\n",
    "    biases -= learning_rate * d_biases\n",
    "    return weights, biases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(y_pred, y_true, input, weights, biases):\n",
    "    # Calculate initial gradient of loss function (Mean Squared Error in this case)\n",
    "    d_loss = 2 * (y_pred - y_true)  # Derivative of MSE = 2 * (predicted - true)\n",
    "\n",
    "    # Backpropagate through the fully connected layer\n",
    "    d_fc_output = d_loss  # As there's no activation after final layer in this example\n",
    "    d_fc_input = np.dot(d_fc_output, weights['fc'].T)\n",
    "    d_fc_weights = np.dot(input['fc'].T, d_fc_output)\n",
    "    d_fc_biases = d_fc_output.sum(axis=0)\n",
    "\n",
    "    # Backpropagate ReLU activation\n",
    "    d_relu = d_fc_input * (input['relu'] > 0)  # Gradient of ReLU is 1 for positive inputs, else 0\n",
    "\n",
    "    # Backpropagate through the convolutional layer\n",
    "    # Assuming conv_backward is defined to handle the specifics of conv layer backprop\n",
    "    d_conv_input, d_conv_weights, d_conv_biases = conv_backward(d_relu, input['conv'], weights['conv'])\n",
    "\n",
    "    # Package gradients in dictionaries to return\n",
    "    gradients = {\n",
    "        'weights': {\n",
    "            'conv': d_conv_weights,\n",
    "            'fc': d_fc_weights\n",
    "        },\n",
    "        'biases': {\n",
    "            'conv': d_conv_biases,\n",
    "            'fc': d_fc_biases\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef initialize_network():\\n    # This should return initialized weights and biases according to your network architecture\\n    weights = {\\n        \\'conv\\': np.random.randn(filter_size, filter_size, input_channels, num_filters),\\n        \\'fc\\': np.random.randn(num_features, num_outputs)\\n    }\\n    biases = {\\n        \\'conv\\': np.random.randn(num_filters),\\n        \\'fc\\': np.random.randn(num_outputs)\\n    }\\n    return weights, biases\\n\\n# Load and preprocess data\\ndata = load_images(\\'./image_folder\\')  # Make sure this function is defined to load images from the directory\\nlabels = load_labels(\\'./json_folder\\')  # Make sure this function is defined to load labels from the directory\\n\\n# Initialize network weights and biases\\nweights, biases = initialize_network()\\n\\n# Training loop\\nlearning_rate = 0.01  # A learning rate of 1 is usually too high; adjust based on your network\\'s performance\\nnum_epochs = 20\\n\\nfor epoch in range(num_epochs):\\n    for i in range(len(data)):\\n        # Forward pass\\n        outputs = forward_pass(data[i], weights, biases)  # Ensure this function correctly computes the forward pass\\n        \\n        # Calculate loss\\n        loss = mean_squared_error(outputs, labels[i])  # Ensure labels are correctly formatted for this loss calculation\\n        \\n        # Backward pass\\n        gradients = backpropagation(outputs, labels[i], data[i], weights, biases)  # Make sure this returns the necessary gradients\\n        \\n        # Update weights\\n        weights, biases = update_weights(weights, biases, gradients, learning_rate)  # Ensure updates are applied correctly\\n        \\n        if i % 100 == 0:\\n            print(f\"Epoch {epoch}, Step {i}, Loss {loss:.4f}\")\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def initialize_network():\n",
    "    # This should return initialized weights and biases according to your network architecture\n",
    "    weights = {\n",
    "        'conv': np.random.randn(filter_size, filter_size, input_channels, num_filters),\n",
    "        'fc': np.random.randn(num_features, num_outputs)\n",
    "    }\n",
    "    biases = {\n",
    "        'conv': np.random.randn(num_filters),\n",
    "        'fc': np.random.randn(num_outputs)\n",
    "    }\n",
    "    return weights, biases\n",
    "\n",
    "# Load and preprocess data\n",
    "data = load_images('./image_folder')  # Make sure this function is defined to load images from the directory\n",
    "labels = load_labels('./json_folder')  # Make sure this function is defined to load labels from the directory\n",
    "\n",
    "# Initialize network weights and biases\n",
    "weights, biases = initialize_network()\n",
    "\n",
    "# Training loop\n",
    "learning_rate = 0.01  # A learning rate of 1 is usually too high; adjust based on your network's performance\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(len(data)):\n",
    "        # Forward pass\n",
    "        outputs = forward_pass(data[i], weights, biases)  # Ensure this function correctly computes the forward pass\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = mean_squared_error(outputs, labels[i])  # Ensure labels are correctly formatted for this loss calculation\n",
    "        \n",
    "        # Backward pass\n",
    "        gradients = backpropagation(outputs, labels[i], data[i], weights, biases)  # Make sure this returns the necessary gradients\n",
    "        \n",
    "        # Update weights\n",
    "        weights, biases = update_weights(weights, biases, gradients, learning_rate)  # Ensure updates are applied correctly\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Step {i}, Loss {loss:.4f}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def initialize_network():\n",
    "    # Define network architecture parameters\n",
    "    filter_size = 3  # example: 3x3 filters\n",
    "    input_channels = 1  # example: grayscale images\n",
    "    num_filters = 32  # example: 32 filters in the first conv layer\n",
    "    num_features = 128  # example: 128 inputs to the fully connected layer\n",
    "    num_outputs = 10  # example: 10 output classes\n",
    "\n",
    "    # Initialize weights and biases\n",
    "    weights = {\n",
    "        'conv': np.random.randn(filter_size, filter_size, input_channels, num_filters) * np.sqrt(2. / (filter_size * filter_size * input_channels)),\n",
    "        'fc': np.random.randn(num_features, num_outputs) * np.sqrt(2. / num_features)\n",
    "    }\n",
    "    biases = {\n",
    "        'conv': np.zeros(num_filters),\n",
    "        'fc': np.zeros(num_outputs)\n",
    "    }\n",
    "    return weights, biases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def initialize_network(filter_size, input_channels, num_filters, num_features, num_outputs):\n",
    "    # Initialize weights and biases with proper scaling\n",
    "    weights = {\n",
    "        'conv': np.random.randn(filter_size, filter_size, input_channels, num_filters) * np.sqrt(2. / (filter_size * filter_size * input_channels)),\n",
    "        'fc': np.random.randn(num_features, num_outputs) * np.sqrt(2. / num_features)\n",
    "    }\n",
    "    biases = {\n",
    "        'conv': np.zeros(num_filters),\n",
    "        'fc': np.zeros(num_outputs)\n",
    "    }\n",
    "    return weights, biases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = initialize_network(3, 1, 32, 128, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "initialize_network() missing 5 required positional arguments: 'filter_size', 'input_channels', 'num_filters', 'num_features', and 'num_outputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Define the network architecture or load it if it's predefined\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m weights, biases \u001b[38;5;241m=\u001b[39m initialize_network()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):  \u001b[38;5;66;03m# Number of epochs\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     data \u001b[38;5;241m=\u001b[39m load_images(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../images\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Ensure this path is correct\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: initialize_network() missing 5 required positional arguments: 'filter_size', 'input_channels', 'num_filters', 'num_features', and 'num_outputs'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the network architecture or load it if it's predefined\n",
    "weights, biases = initialize_network()\n",
    "\n",
    "for epoch in range(20):  # Number of epochs\n",
    "    data = load_images(\"../images\")  # Ensure this path is correct\n",
    "    labels = load_labels(\"../json_labeling\")  # Ensure this path is correct and data format matches expected input\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        try:\n",
    "            # Forward pass\n",
    "            outputs = forward_pass(data[i], weights, biases)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = mean_squared_error(outputs, labels[i])\n",
    "\n",
    "            # Backward pass\n",
    "            gradients = backpropagation(outputs, data[i], labels[i], weights, biases)\n",
    "\n",
    "            # Update weights\n",
    "            learning_rate = 0.01  # A more typical value for learning rate\n",
    "            weights, biases = update_weights(weights, biases, gradients, learning_rate)\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Step {i}, Loss {loss}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred at epoch {epoch}, step {i}: {str(e)}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
